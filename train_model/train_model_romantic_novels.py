# -*- coding: utf-8 -*-
"""train_model_romantic_novels.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13L2t5mhZ24ZwHdHFzoruCBVywo7btoTr

LDA Topic Modeling

This code is for a topic modeling experiment using the Latent Dirichlet Allocation (LDA) model on a dataset of text files. It first downloads FastText pre-trained word vectors. It loads a given dataset and preprocesses it with count vectorization. After that, it iterates over a parameter grid and runs LDA for each combination of hyperparameters, tracking the coherence and perplexity for each run. Then it selects and saves the best LDA model based on either the coherence score or perplexity score. Lastly, it loads the saved LDA model and prints the most coherent topic and its top words.
"""

from google.colab import drive
drive.mount('/content/drive')

# Import necessary libraries and modules
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from itertools import combinations
from pathlib import Path
import numpy as np
import os
import requests
import gzip
from time import time
from random import shuffle
from os.path import exists, join
from sklearn.metrics import pairwise_distances
from sklearn.model_selection import ParameterGrid
import matplotlib.pyplot as plt
import pickle

# Define hyperparameters for LDA and plotting
n_samples = 2000
n_features = 5000
n_components = 15
max_df = 0.7
min_df = 20
init = "nndsvda"
n_top_words = 20

# Function to download FastText word vectors
def download_ft_vectors():
    # Define the download path for the word vectors
    path = "https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.vec.gz"

    # Create a directory to store the downloaded file if it doesn't exist
    processed_dir = join(os.getcwd(), 'data')
    Path(processed_dir).mkdir(exist_ok=True, parents=True)

    # Define the local path for the downloaded file
    gz_file = join(processed_dir, path.split('/')[-1])

    # Check if the file exists, if not, download it
    if not exists(gz_file):
        with open(gz_file, "wb") as f:
            f.write(requests.get(path).content)

    print("Word vectors available!")
    return gz_file

# Function to extract feature vectors from FastText file
def get_feature_vectors(gz_file, feature_names):
    m = []
    sorted_feature_names = []
    first_line = True
    c = 0
    # Open the FastText file
    with gzip.open(gz_file, 'rt') as fin:
        # Iterate over each line in the file
        for l in fin:
            # Skip the first line (contains metadata)
            if first_line:
                first_line = False
                continue
            # Split the line into word and feature vector
            fs = l.rstrip('\n').split()
            word = fs[0]
            # If the word is in the list of feature names, append its feature vector to 'm'
            if word in feature_names:
                vec = np.array([float(v) for v in fs[1:]])
                sorted_feature_names.append(word)
                m.append(vec)
            c += 1
            # Break after the top 50k words for efficiency
            if c > 50000:
                break
    return sorted_feature_names, np.array(m)

# Function to compute coherence score for given top words
def compute_coherence(top_words, sorted_feature_list, m):
    feats_idx = [sorted_feature_list.index(w) for w in top_words if w in sorted_feature_list]
    truncated_m = m[feats_idx, :]
    cosines = 1 - pairwise_distances(truncated_m, metric="cosine") / 2
    return np.mean(cosines)

# Function to compute and return coherence score for all topics
def return_coherence_list(model, feature_names, n_top_words, sorted_feature_list, m):
    coherences = []
    for topic_idx, topic in enumerate(model.components_):
        print("---> Computing coherence for topic", topic_idx)
        top_features_ind = topic.argsort()[: -n_top_words - 1: -1]
        top_features = [feature_names[i] for i in top_features_ind]
        coherence = compute_coherence(top_features, sorted_feature_list, m)
        print(top_features, coherence)
        coherences.append(coherence)
    return coherences

# Function to plot the top words for each topic
def plot_top_words(model, feature_names, n_top_words, title):
    fig, axes = plt.subplots(4, 5, figsize=(30, 15), sharex=True)
    axes = axes.flatten()
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[: -n_top_words - 1: -1]
        top_features = [feature_names[i] for i in top_features_ind]
        weights = topic[top_features_ind]
        ax = axes[topic_idx]
        ax.barh(top_features, weights, height=0.3)
        ax.set_title(f"Topic {topic_idx + 1}", fontdict={"fontsize": 14})
        ax.invert_yaxis()
        ax.tick_params(axis="both", which="major", labelsize=8)
        for i in "top right left".split():
            ax.spines[i].set_visible(False)
        fig.suptitle(title, fontsize=14)
    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)
    plt.show()

# Function to transform data using CountVectorizer
def get_tfs(data=None, nfeats=None, max_df=None, min_df=None):
    print("Extracting tf features for LDA...")
    tf_vectorizer = CountVectorizer(max_df=max_df, min_df=min_df, max_features=nfeats, stop_words="english")
    tfs = tf_vectorizer.fit_transform(data)
    return tf_vectorizer, tfs

# Function to fit an LDA model to the data
def run_LDA(data=None, max_df=None, min_df=None, nfeats=None, n_components=None):
    tf_vectorizer, tfs = get_tfs(data=data, nfeats=nfeats, max_df=max_df, min_df=min_df)
    print("\nFitting LDA models with tf features, n_samples=%d and n_features=%d..." % (n_samples, nfeats))
    lda = LatentDirichletAllocation(n_components=n_components, max_iter=5, learning_method="online", learning_offset=10.0, random_state=0)
    t0 = time()
    lda.fit(tfs)
    print("done in %0.3fs." % (time() - t0))
    perplexity = lda.perplexity(tfs)
    return tf_vectorizer, lda, perplexity

# Download word vectors
print("Downloading word vectors...")
gz_path = download_ft_vectors()

# Function to load text data from a directory
def load_data(directory, doc_length=200):
    docs = []
    for path in Path(directory).iterdir():
        if path.is_file() and path.suffix == '.txt':
            with open(path) as f:
                contents = f.read()
                docs += [''.join(contents[start:start + doc_length]) for start in range(0, len(contents), doc_length)]
    shuffle(docs)
    return docs

directory = "/content/drive/MyDrive/high_lit_topics/nokens_noun"

# Download word vectors
print("Downloading word vectors...")
gz_path = download_ft_vectors()

# Define a grid of hyperparameters to search
param_grid = {
    'doc_length': [50, 100, 200],
    'max_df': [0.5, 0.7],
    'min_df': [20, 30],   # changed min_df to 20 instead of 0.2
    'n_features': [1000],
    'n_components': [30, 35, 40, 45]
}

# Transform param_grid into a list of dictionaries
grid = list(ParameterGrid(param_grid))

perplexities = []
coherences = []

# Loop through all combinations of hyperparameters
for p in grid:
    print("\n", p)
    print("Loading dataset...")
    data = load_data(directory=directory, doc_length=p['doc_length'])
    data_samples = data[:n_samples]
    tf_vectorizer, lda, perplexity = run_LDA(data=data_samples, max_df=p['max_df'], min_df=p['min_df'],
                                             nfeats=p['n_features'], n_components=p['n_components'])
    tf_feature_names = tf_vectorizer.get_feature_names_out()
    sorted_feature_names, m = get_feature_vectors(gz_path, tf_feature_names)
    print("Perplexity", perplexity)
    perplexities.append(perplexity)
    coherence = np.mean(return_coherence_list(lda, tf_feature_names, n_top_words, sorted_feature_names, m))
    print("Coherence", coherence)
    coherences.append(coherence)

best_results = {"coherence": np.argmax(coherences), "perplexity": np.argmin(perplexities)}

# Sort the best results by coherence in descending order
sorted_best_results = sorted(best_results.items(), key=lambda x: x[1], reverse=True)

# Print the top 3 models
for result_name, best in sorted_best_results[:1]:
    p = grid[best]
    print(directory.split("/")[-1], f"\n\n[{result_name.upper()}] BEST HYPERPARAMETERS:", grid[best], "COHERENCE:",
          coherences[best], "PERPLEXITY:", perplexities[best])

    # Save the best model
with open("/content/drive/MyDrive/best_model_cla_literature_nouns.pkl", "wb") as f:
    pickle.dump(lda, f)

# Load the best model
with open('/content/drive/MyDrive/best_model_cla_literature_nouns.pkl', "rb") as f:
    lda_loaded = pickle.load(f)

tf_feature_names = tf_vectorizer.get_feature_names_out()
sorted_feature_names, m = get_feature_vectors(gz_path, tf_feature_names)
cohs = return_coherence_list(lda_loaded, tf_feature_names, n_top_words, sorted_feature_names, m)
best_topic_idx = np.argmax(cohs)
best_topic = cohs[best_topic_idx]
top_words_ind = lda_loaded.components_[best_topic_idx].argsort()[: -n_top_words - 1: -1]
top_words = [tf_feature_names[i] for i in top_words_ind]
print("Best Topic:", best_topic)
print("Top Words:", top_words)